# Technical Implementation Details - Workforce Intelligence Dashboard v2.0

## Architecture Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              STREAMLIT APPLICATION LAYER                â”‚
â”‚  (UI Components, Navigation, Interaction Handling)      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚          VISUALIZATION & ANALYTICS LAYER                â”‚
â”‚  (Plotly Figures, DataFrames, Calculations)             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  - Sankey Diagrams       â”‚  - Moving Average Trends     â”‚
â”‚  - Heatmaps              â”‚  - Box & Whisker Plots       â”‚
â”‚  - Gauge Charts          â”‚  - Skills Distribution       â”‚
â”‚  - Bar Charts            â”‚  - Doughnut Charts           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚          DATA PROCESSING & LOGIC LAYER                  â”‚
â”‚  (Filtering, Aggregation, Recommendations)              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  - PersonalizedRecommender Class                        â”‚
â”‚  - Skill Gap Calculations                               â”‚
â”‚  - Labor Shortage Index                                 â”‚
â”‚  - Sector Growth Scoring                                â”‚
â”‚  - Data Export Functions                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚             DATA LOADING & CACHING LAYER                â”‚
â”‚  (Pandas DataFrame Operations, st.cache_data)           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  - CSV Loading (SGJobData.csv)                          â”‚
â”‚  - Data Preprocessing                                   â”‚
â”‚  - JSON Parsing (Categories)                            â”‚
â”‚  - Date Handling                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                DATA SOURCE (1M+ Records)                â”‚
â”‚  Singapore Job Market Data - 20+ Columns                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Module-by-Module Breakdown

### 1. **Imports & Configuration**

```python
# Core Framework
import streamlit as st

# Data Processing
import pandas as pd
import numpy as np

# Visualization
import plotly.graph_objects as go
import plotly.express as px
import altair as alt

# Utilities
from datetime import datetime, timedelta
import json
from typing import Dict, List, Tuple, Optional
import warnings
from io import BytesIO

# Optional Exports
try:
    from reportlab.lib.pagesizes import letter
    from reportlab.platypus import SimpleDocTemplate, Table, Paragraph
    PDF_AVAILABLE = True
except ImportError:
    PDF_AVAILABLE = False

try:
    import openpyxl
    from openpyxl.styles import PatternFill, Font
    EXCEL_AVAILABLE = True
except ImportError:
    EXCEL_AVAILABLE = False
```

**Design Pattern:** Graceful degradation - app works with core libraries, enhanced with optional ones.

---

### 2. **CSS Theming & Styling**

```python
st.markdown("""<style>
    * { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto; }
    .main { background: linear-gradient(180deg, #0f1419 0%, #1a1f2e 100%); }
    .stMetric { 
        background: linear-gradient(135deg, #1e2936 0%, #243447 100%);
        padding: 1.75rem;
        border-radius: 1rem;
        box-shadow: 0 4px 15px rgba(59, 130, 246, 0.15);
        border: 2px solid #3b82f6;
        transition: all 0.3s ease;
    }
    .stMetric:hover { 
        box-shadow: 0 6px 20px rgba(59, 130, 246, 0.25);
        transform: translateY(-2px);
    }
</style>""", unsafe_allow_html=True)
```

**Key Features:**
- System font stack for performance
- Gradient backgrounds for depth
- Box shadows for elevation
- Hover transforms for interactivity
- Consistent padding throughout (1.75rem base unit)

---

### 3. **PersonalizedRecommender Class**

```python
class PersonalizedRecommender:
    def __init__(self, df: pd.DataFrame):
        self.df = df
        self.all_skills, self.emerging_skills = calculate_skill_gaps(df)
        self.skill_percentiles = self._calculate_skill_percentiles()
    
    def _calculate_skill_percentiles(self) -> Dict[str, float]:
        """
        For each skill, find the 90th percentile salary.
        
        Logic:
        1. Filter jobs containing the skill
        2. Calculate np.percentile(..., 90)
        3. Store mapping: skill -> salary_90th
        
        Used to show "Learning X could increase salary to Y"
        """
    
    def get_recommendations(self, user_skills, desired_salary) -> Dict:
        """
        Returns:
        {
            'upskill_opportunities': [
                {
                    'skill': 'Cloud',
                    'potential_increase': 'SGD 2,500',
                    'effort': 'High'
                },
                ...
            ],
            'high_growth_sectors': [
                {
                    'sector': 'Information Technology',
                    'growth_score': 85.3,
                    'match_fit': 'Perfect âœ“'
                },
                ...
            ],
            'salary_potential': 9500.0,
            'skill_gap_premium': {
                'Cloud': 2500.0,
                'AI/ML': 3000.0
            }
        }
        """
```

**Key Algorithms:**

1. **Skill Gap Premium Calculation:**
   ```python
   for skill in emerging_skills:
       salary_90th = df[df['title'].contains(skill)]['salary'].quantile(0.9)
       premium = salary_90th - user_desired_salary
       recommendations['skill_gap_premium'][skill] = premium
   ```

2. **Sector Growth Score:**
   ```python
   growth = (volume_pct * 0.4) + (salary_premium * 0.3) + (engagement * 0.3)
   # Volume: % of market posting count
   # Salary Premium: avg_sector_salary / avg_market_salary
   # Engagement: avg views per posting normalized
   ```

3. **Skill Match Calculation:**
   ```python
   match_count = sum(1 for skill in user_skills 
                     if sector_df['title'].contains(skill))
   match_pct = (match_count / len(user_skills)) * 100
   
   if match_pct >= 75: match_fit = "Perfect âœ“"
   elif match_pct >= 50: match_fit = "Good â†’"
   else: match_fit = "Developing"
   ```

---

### 4. **Visualization Functions**

#### A. Sankey Diagram (Career Pathways)
```python
def create_skill_sankey_diagram(df, user_skills):
    """
    Creates flow diagram:
    User Skills (Blue) â†’ Emerging Skills (Green) â†’ Sectors (Orange)
    
    Parameters:
    - User skills: nodes on left (source)
    - Emerging skills: nodes in middle
    - Growth sectors: nodes on right (destination)
    
    Links: Weighted by skill demand
    
    Returns: go.Figure with Sankey
    """
    # Build labels list combining all node types
    labels = user_skills + emerging_skills + sectors
    
    # Build source/target indices
    for skill in user_skills:
        for emerg in emerging_skills:
            source.append(labels.index(skill))
            target.append(labels.index(emerg))
            value.append(demand[emerg] // 10)
    
    return go.Figure(data=[go.Sankey(...)])
```

**Customization Points:**
- Node colors: Blue â†’ Green â†’ Orange progression
- Link opacity: Show demand flow proportions
- Hover templates: Detailed skill information
- Interactive: Click/drag nodes to reorganize

---

#### B. Trend with Moving Average
```python
def create_trend_with_ma(df, window=3):
    """
    Combines 3 traces:
    1. Historical data line+markers (blue)
    2. Moving average (green dashed)
    3. Confidence band (shaded gray)
    
    Calculation:
    - MA: rolling(window).mean()
    - Upper band: MA * 1.15
    - Lower band: MA * 0.85
    
    Use Case: Government forecasting
    Interpretation: Green line = projected direction
    """
    fig = go.Figure()
    
    # Confidence band
    fig.add_trace(go.Scatter(
        y=upper_band, fill=None, name='Upper Band'
    ))
    fig.add_trace(go.Scatter(
        y=lower_band, fill='tonexty', name='Confidence', 
        fillcolor='rgba(59, 130, 246, 0.1)'
    ))
    
    # Historical + MA
    fig.add_trace(go.Scatter(
        y=actual, mode='lines+markers', name='Actual'
    ))
    fig.add_trace(go.Scatter(
        y=ma, mode='lines', name='MA', line=dict(dash='dash')
    ))
```

**Mathematical Basis:**
```
MA[t] = (value[t-1] + value[t] + value[t+1]) / 3  (window=3)
Upper[t] = MA[t] Ã— 1.15  (15% confidence interval)
Lower[t] = MA[t] Ã— 0.85
```

---

#### C. Skills Distribution (Doughnut)
```python
def create_skill_distribution_pie(df):
    """
    Top 8 skills by demand count.
    
    Layout: Doughnut (hole=0.3)
    - Reduces visual weight
    - Space for center label
    - Professional appearance
    
    Colors: Distinct palette for each skill
    """
    fig = go.Figure(data=[go.Pie(
        labels=skill_names,
        values=demand_counts,
        hole=0.3,  # Doughnut style
        marker=dict(colors=['#3b82f6', '#10b981', ...]),
        textposition='inside',
        textinfo='label+percent'
    )])
```

---

### 5. **Data Export Functions**

#### Excel Export Architecture
```python
def create_excel_export(df, persona):
    """
    Creates BytesIO object with 3 sheets:
    
    Sheet 1: Summary
    - Key metrics (count, avg salary, engagement)
    - 1 row Ã— 7 columns
    
    Sheet 2: Sector Analysis
    - Aggregated by sector
    - 10 rows Ã— 8 columns (top sectors)
    
    Sheet 3: Job Listings
    - Full detail (unsampled)
    - N rows Ã— 8 columns
    
    Formatting:
    - Blue header (HexColor #3b82f6)
    - Auto-width columns (max 50 chars)
    - Currency formatting for salary
    - Center alignment for numbers
    """
    output = BytesIO()
    
    with pd.ExcelWriter(output, engine='openpyxl') as writer:
        summary_stats.to_excel(writer, sheet_name='Summary')
        sector_analysis.to_excel(writer, sheet_name='Sector Analysis')
        job_details.to_excel(writer, sheet_name='Job Listings')
        
        # Apply formatting to sheets
        for sheet in writer.sheets.values():
            for column in sheet.columns:
                # Calculate max width
                max_length = max(len(str(cell)) for cell in column)
                sheet.column_dimensions[col_letter].width = min(max_length + 2, 50)
    
    output.seek(0)
    return output
```

**Key Design:**
- Uses `pd.ExcelWriter` for multi-sheet output
- `openpyxl` for post-processing styles
- BytesIO for in-memory file handling
- Conditional execution (only if EXCEL_AVAILABLE)

---

#### PDF Export Architecture
```python
def create_pdf_export(df, persona, recommendations=None):
    """
    Creates PDF with sections:
    1. Title (24pt, blue)
    2. Executive Summary (table of stats)
    3. Sector Breakdown (top 10 table)
    4. Personalized recs (if individual)
    
    Uses ReportLab:
    - SimpleDocTemplate (standard letter)
    - Table + TableStyle for styled tables
    - Paragraph for rich text
    - Spacer for layout
    
    Output: BytesIO ready for download
    """
    output = BytesIO()
    doc = SimpleDocTemplate(output, pagesize=letter)
    
    elements = []
    styles = getSampleStyleSheet()
    
    # Title
    title = Paragraph(f"Report - {persona}", styles['Heading1'])
    elements.append(title)
    
    # Summary table
    table_data = [['Metric', 'Value'], ['Postings', len(df)], ...]
    table = Table(table_data, colWidths=[2, 3])
    table.setStyle(TableStyle([
        ('BACKGROUND', (0,0), (-1,0), colors.HexColor('#3b82f6')),
        ('TEXTCOLOR', (0,0), (-1,0), colors.whitesmoke),
        ...
    ]))
    elements.append(table)
    
    # Build PDF
    doc.build(elements)
    output.seek(0)
    return output
```

---

### 6. **Data Processing Pipeline**

#### Loading & Caching
```python
@st.cache_data
def load_data(filepath: str) -> pd.DataFrame:
    """
    Cached CSV loading with error handling.
    
    st.cache_data decorator:
    - Loads once on app start
    - Reuses in memory across reruns
    - Invalidates if function code changes
    - Parameters: None (same file always)
    """
    df = pd.read_csv(filepath, on_bad_lines='skip', encoding='utf-8')
    return df
```

#### Preprocessing
```python
@st.cache_data
def preprocess_data(df: pd.DataFrame) -> pd.DataFrame:
    """
    Transformations:
    1. Date conversion (3 cols)
    2. Time extraction (year, month, period)
    3. Numeric handling (salary, experience)
    4. Category extraction (JSON â†’ string)
    5. Fillna for categorical cols
    
    Performance: Vectorized pandas operations
    Output: Clean DataFrame ready for analysis
    """
```

#### Aggregation
```python
def calculate_labor_shortage_index(df) -> Dict[str, float]:
    """
    For each sector:
    1. Calculate 4 components (0-100 scale)
       - Posting volume: len(sector) / total * 200 capped at 100
       - Views per posting: 100 - (avg_views / 100) * 50
       - App ratio: 100 - (avg_apps / 5) * 50
       - Salary level: (avg_salary / 5000) * 50
    
    2. Weighted composite:
       Index = (posting * 0.3) + (views * 0.2) + (apps * 0.3) + (salary * 0.2)
    
    3. Sort descending (highest shortage first)
    
    Returns: Dict[sector_name, shortage_index]
    """
```

---

### 7. **Sidebar Organization**

#### Filter Structure
```
SIDEBAR
â”œâ”€â”€ ğŸ‘¤ SELECT YOUR PERSONA (radio)
â”‚
â”œâ”€â”€ ğŸ›ï¸ FILTERS (expander)
â”‚   â”œâ”€â”€ Time Period
â”‚   â”‚   â””â”€â”€ Year multiselect
â”‚   â”œâ”€â”€ Industry & Employment
â”‚   â”‚   â”œâ”€â”€ Sector multiselect
â”‚   â”‚   â””â”€â”€ Employment type multiselect
â”‚
â”œâ”€â”€ ğŸ’¼ YOUR PROFILE / ğŸ“Š MACRO / ğŸ¯ RECRUITMENT (conditional expander)
â”‚   â””â”€â”€ Persona-specific controls
â”‚
â””â”€â”€ â„¹ï¸ DATASET INFO (collapsible)
    â”œâ”€â”€ Last Updated
    â”œâ”€â”€ Records Displayed
    â”œâ”€â”€ Total Records
    â”œâ”€â”€ Sector Count
    â””â”€â”€ Company Count
```

**Benefits:**
- Collapsible sections reduce cognitive load
- Logical grouping by purpose
- Conditional display based on persona
- Metadata easily accessible

---

### 8. **Persona-Specific Layouts**

#### Individual Dashboard Structure
```
1. Header + 3 KPI Cards
   - Salary Potential (90th percentile)
   - Skills to Acquire (count)
   - Growth Sectors (count)

2. Sankey Diagram (Career Paths)

3. Upskill Opportunities (3 cards, horizontal)
   - Skill name
   - Salary increase potential
   - Learning difficulty

4. Growth Sectors Table
   - Rank, Sector, Score, Match, Salary

5. Skills Distribution Chart (doughnut)

6. Salary Distribution Chart (by sector)

7. Download Options (CSV + PDF)
```

#### Government Dashboard Structure
```
1. Header + 4 KPI Cards
   - Total Postings + %
   - Active Sectors
   - Avg Salary + YoY
   - Applications + per-posting ratio

2. Labor Shortage Index
   - Selector + Gauge chart
   - Top 5 critical areas

3. Employment Heatmap

4. Job Growth Trends (with MA)

5. Sector Statistics Table (comprehensive)

6. Export Options (CSV + Excel + PDF)
```

#### Recruiter Dashboard Structure
```
1. Header + 4 KPI Cards
   - Open Positions
   - Avg Applications
   - Top Hiring Company
   - Competitive Rate

2. High-Velocity Talent Pools
   - Bar chart (salary color-coded)
   - Metrics sidebar

3. Salary Benchmarking (box plot)

4. Skills Demand Heatmap (horizontal bar)

5. Top Hiring Companies (bar chart)

6. Export Options (CSV + Excel)
```

---

### 9. **Common Features**

#### Tab Navigation
```python
tab1, tab2, tab3, tab4 = st.tabs([
    "ğŸ” Job Search",
    "ğŸ­ Sector Deep-Dive", 
    "ğŸ“ˆ Trend Analysis",
    "ğŸ“¥ Download Data"
])

with tab1:
    # Full-text search functionality
    search_term = st.text_input(...)
    if search_term:
        results = filtered_df[
            filtered_df['title'].str.contains(search_term, case=False)
        ]

with tab2:
    # Single sector analysis
    sector = st.selectbox(...)
    sector_data = filtered_df[filtered_df['sector'] == sector]
    # Show top positions, companies, salary stats

with tab3:
    # Customizable trend analysis
    metric = st.selectbox(['Job Postings', 'Salary', 'Apps', 'Views'])
    window = st.slider(2, 6, 3)
    # Build trend plot with user-selected MA window

with tab4:
    # Multi-format export
    # CSV always available
    # Excel/PDF conditional on libraries
```

---

## Performance Optimization

### Caching Strategy
```python
@st.cache_data
def load_data(filepath):
    """Cache entire CSV in memory (loaded once)"""

@st.cache_data
def preprocess_data(df):
    """Cache cleaned DF (dependent on load_data)"""

# After caching layer, all filtering uses vectorized pandas:
filtered_df = df[df['year'].isin(selected_years)]  # Vectorized mask
filtered_df = filtered_df[filtered_df['sector'].isin(sectors)]  # Chained

# Avoid per-row operations - use groupby instead:
sector_stats = df.groupby('sector').agg({
    'salary': ['mean', 'min', 'max'],
    'postings': 'count'
})  # Single vectorized operation vs. N loops
```

**Benchmark:** 1M record CSV loads in 2-3 seconds, cached in memory thereafter.

---

### Memory Management
```python
# Use BytesIO for exports instead of disk files
export_buffer = BytesIO()  # In-memory only
pd.ExcelWriter(export_buffer, engine='openpyxl')  # Writes to buffer
export_buffer.seek(0)  # Reset pointer
st.download_button(data=export_buffer)  # Send to browser

# Cleanup: BytesIO objects garbage collected after function return
```

---

## Error Handling & Validation

```python
# Empty dataset handling
if len(filtered_df) == 0:
    st.warning("âš ï¸ No jobs match filters")
    filtered_df = df.copy()  # Reset to full dataset

# Safe JSON parsing
def extract_primary_category(cat_str):
    try:
        if pd.isna(cat_str) or cat_str == '':
            return 'Other'
        categories = json.loads(cat_str.replace('""', '"'))
        if isinstance(categories, list) and len(categories) > 0:
            return categories[0].get('category', 'Other')
    except:
        return 'Other'  # Graceful fallback

# Optional library check
if PDF_AVAILABLE:
    # PDF export available
else:
    st.warning("PDF export unavailable - install reportlab")

# Null-safe calculations
avg_salary = filtered_df['salary'].mean() if len(filtered_df) > 0 else 0
```

---

## CSS Classes & Styling Reference

```css
/* Metric cards with hover effects */
.stMetric {
    background: linear-gradient(135deg, #1e2936 0%, #243447 100%);
    padding: 1.75rem;
    border-radius: 1rem;
    box-shadow: 0 4px 15px rgba(59, 130, 246, 0.15);
    border: 2px solid #3b82f6;
    transition: all 0.3s ease;
}

.stMetric:hover {
    box-shadow: 0 6px 20px rgba(59, 130, 246, 0.25);
    transform: translateY(-2px);
}

/* Recommendation boxes */
.recommendation-box {
    background: linear-gradient(135deg, #065f46 0%, #047857 100%);
    border-left: 6px solid #10b981;
    padding: 1.5rem;
    border-radius: 0.75rem;
    margin: 1.25rem 0;
    color: #d1fae5;
}

/* Shortage index boxes */
.shortage-index {
    background: linear-gradient(135deg, #7c2d12 0%, #92400e 100%);
    border-left: 6px solid #fb923c;
    padding: 1.5rem;
    border-radius: 0.75rem;
    color: #fed7aa;
}

/* Sidebar styling */
section[data-testid="stSidebar"] {
    background: linear-gradient(180deg, #0f1419 0%, #1a1f2e 100%);
}

/* Tab styling */
.stTabs [role="tablist"] button[aria-selected="true"] {
    color: #ffffff !important;
    border-bottom: 3px solid #3b82f6 !important;
}
```

---

## Testing & Validation

```python
# Unit test examples
def test_skill_percentile_calculation():
    df = pd.DataFrame({
        'title': ['Python Dev', 'Python Dev', 'Python Engineer'],
        'salary': [5000, 6000, 8000]
    })
    recommender = PersonalizedRecommender(df)
    assert recommender.skill_percentiles['Python'] == 8000  # 90th percentile

def test_labor_shortage_index():
    # Sector with high posting volume, low views = high shortage
    df = sample_sector_data()
    index = calculate_labor_shortage_index(df)
    assert index['SectorA'] > 70  # Critical shortage

def test_empty_filter_handling():
    # Ensure no crash with zero results
    filtered_df = df[df['sector'] == 'NonExistent']
    assert len(filtered_df) == 0
    # App should show warning and reset to full df
```

---

## Deployment Considerations

### Requirements.txt
```
streamlit==1.28.1
pandas==2.0.3
numpy==1.24.3
plotly==5.14.0
altair==5.0.1
reportlab==4.0.4
openpyxl==3.10.0
```

### Docker Deployment
```dockerfile
FROM python:3.10-slim

WORKDIR /app

COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

COPY . .
COPY SGJobData.csv .

EXPOSE 8501

CMD ["streamlit", "run", "SGJOB_Dashboard.py", "--logger.level=info"]
```

### Environment Variables
```bash
# For Streamlit server
STREAMLIT_SERVER_PORT=8501
STREAMLIT_SERVER_ADDRESS=0.0.0.0
STREAMLIT_LOGGER_LEVEL=info

# For app configuration
CSV_PATH=/app/SGJobData.csv
PERSONA_DEFAULT=Individual
```

---

## Future Extensibility

### Adding New Visualizations
```python
def create_custom_visualization(df):
    """Template for new viz function"""
    fig = go.Figure()
    
    fig.add_trace(go.Scatter(...))
    
    fig.update_layout(
        plot_bgcolor='#0f1419',
        paper_bgcolor='#0f1419',
        font=dict(color='#e0e7ff'),
        ...
    )
    
    return fig

# Register in appropriate persona
st.plotly_chart(create_custom_visualization(filtered_df))
```

### Adding New Recommender Logic
```python
class AdvancedRecommender(PersonalizedRecommender):
    """Extend base recommender with new algorithms"""
    
    def get_collaborative_recommendations(self, user_id):
        """Recommend based on similar users"""
        pass
    
    def get_course_recommendations(self):
        """Suggest learning paths"""
        pass
```

---

**Last Updated:** February 2026  
**Architecture Version:** 2.0.0  
**Status:** Production Ready âœ…
